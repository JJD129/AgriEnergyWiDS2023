{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0599a33e",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "Prediction task: forecast sub-seasonal temperatures (over a two-week period) within the United States.\n",
    "\n",
    "Data: weather and climate information for various US locations. Start dates for the two-week observation, as well as the forecasted temperature and precipitation from a number of weather forecast models (we will reveal the source of our dataset after the competition closes)\n",
    "\n",
    "Each row correspons to a single location and a single start date for the two-week period.\n",
    "\n",
    "Target: contest-tmp2m-14d__tmp2m, the arithmetic mean of the max and min observed temperature over the next 14 days for each location and start date, computed as (measured max temperature + measured mini temperature) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a845c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading library\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score)\n",
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train = pd.read_csv('train_data.csv')\n",
    "test = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be1f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e344b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary stats and putting in spreadsheet\n",
    "# train.describe().T.to_csv(\"train_describe.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ad67a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a4e9f0",
   "metadata": {},
   "source": [
    "object items are:\n",
    "* startdate\n",
    "* climateregions__climateregion\n",
    "\n",
    "int64 items are:\n",
    "* index\n",
    "* mjo1d__phase\n",
    "* mei__meirank\n",
    "* mei__nip\n",
    "\n",
    "rest are float64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5151f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0255bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast policy startdate to datetime\n",
    "train['startdate'] = pd.to_datetime(train['startdate'])\n",
    "test['startdate'] = pd.to_datetime(train['startdate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search = pd.DataFrame.duplicated(train)\n",
    "# print(search[search == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb63229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search = pd.DataFrame.duplicated(test)\n",
    "# print(search[search == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f919973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing\n",
    "null_columns=train.columns[train.isnull().sum() != 0]\n",
    "print(train[null_columns].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98ccde1",
   "metadata": {},
   "source": [
    "There are features that share the same missing values. For example, ccsm30 has 15934 missing values and so do nmme0-prate-34w__ccsm30, nmme0-prate-56w__ccsm30, nmme0-prate-56w__ccsm30, and nmme0-tmp2m-34w__ccsm30. ccsm30 values are forecasts from weather models and those values could be used to forecast prate-34w__ccsm30, nmme0-prate-56w__ccsm30, nmme0-prate-56w__ccsm30, and nmme0-tmp2m-34w__ccsm30. The same thing might be the case for other features that used forecasted values to extrapolate their measurements.\n",
    "\n",
    "*might need to check for spurious correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5f7b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing\n",
    "# null_columns=test.columns[test.isnull().sum() != 0]\n",
    "# print(test[null_columns].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21243f13",
   "metadata": {},
   "source": [
    "Test dataset doesn't have any null columns so they don't have to treat for nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d64095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['climateregions__climateregion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7202a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test['climateregions__climateregion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9f36e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,10))\n",
    "# sns.lineplot(data=train, x='startdate', y='contest-tmp2m-14d__tmp2m',hue='climateregions__climateregion', ci=None)\n",
    "# plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c260b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # top 500 getting columns with a certain naming convention into a new df\n",
    "# df_500 = train.head(500)\n",
    "# df_500_nmme = df_500[[col for col in df_500.columns if 'nmme0-tmp2m-34w' in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a6a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # concat new df to add a startdate and targe variable, resetting index to be startdate, so x axis label is the startdate\n",
    "# df_500_nmme = pd.concat([df_500_nmme, df_500[['contest-tmp2m-14d__tmp2m','startdate']]], axis=1)\n",
    "# df_500_nmme = df_500_nmme.set_index('startdate')\n",
    "\n",
    "# plt.figure(figsize=(20,15))\n",
    "# #sns.lineplot(data=df_500_nmme, x='startdate', y='contest-tmp2m-14d__tmp2m', ci=None)\n",
    "\n",
    "# #Plot all the columns in a single plot\n",
    "# df_500_nmme.plot(kind='line')\n",
    "\n",
    "# #Add a title and labels for the x and y axis\n",
    "# plt.title(f'Temperature comparison (all nmme0-tmp2m-34w columns vs contest-tmp2m-14d__tmp2m)')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Temperature (Â°C)')\n",
    "# plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "\n",
    "# #Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f34f3f",
   "metadata": {},
   "source": [
    "Leonie made a feature location based on lat/lon coordinates, which makes sense if we want to better understand seasonal temps. Sub seasonal temps vary between locations across the continental US.\n",
    "https://www.kaggle.com/code/iamleonie/wids-datathon-2023-forecasting-with-lgbm#Feature-Engineering\n",
    "\n",
    "However there is an issue with the lat/lons. The test dataset has different locations. The issue was resolved by Flavia in her notebook where the issue stems from a rounding difference in the test data. She suggests truncating the train lat/lon. https://www.kaggle.com/code/flaviafelicioni/wids-2023-different-locations-train-test-solved#Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f841b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test data\n",
    "all_df = pd.concat([train, test], axis=0)\n",
    "\n",
    "# Create new feature\n",
    "all_df['loc_group'] = all_df.groupby(['lat','lon']).ngroup()\n",
    "display(all_df)\n",
    "\n",
    "print(f'{all_df.loc_group.nunique()} unique locations')\n",
    "\n",
    "# Split back up\n",
    "train = all_df.iloc[:len(train)]\n",
    "test = all_df.iloc[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ddf9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Locations in train that are not in test')\n",
    "print([c for c in train.loc_group.unique() if c not in test.loc_group.unique()])\n",
    "\n",
    "print('Locations in test that are not in train')\n",
    "print([c for c in test.loc_group.unique() if c not in train.loc_group.unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e47c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 14\n",
    "\n",
    "train_df.loc[:,'lat']=round(train_df.lat,scale)\n",
    "train_df.loc[:,'lon']=round(train_df.lon,scale)\n",
    "\n",
    "test_df.loc[:,'lat']=round(test_df.lat,scale)\n",
    "test_df.loc[:,'lon']=round(test_df.lon,scale)\n",
    "\n",
    "# Concatenate train and test data\n",
    "all_df = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "# Create new feature\n",
    "all_df['loc_group'] = all_df.groupby(['lat','lon']).ngroup()\n",
    "display(all_df)\n",
    "\n",
    "print(f'{all_df.loc_group.nunique()} unique locations')\n",
    "\n",
    "# Split back up\n",
    "train_df = all_df.iloc[:len(train_df)]\n",
    "test_df = all_df.iloc[len(train_df):]\n",
    "\n",
    "print('Locations in train that are not in test')\n",
    "print([c for c in train_df.loc_group.unique() if c not in test_df.loc_group.unique()])\n",
    "\n",
    "print('Locations in test that are not in train')\n",
    "print([c for c in test_df.loc_group.unique() if c not in train_df.loc_group.unique()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52103bc5",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf73c315",
   "metadata": {},
   "source": [
    "From leonie:\n",
    "\n",
    "Often times in Machine Learning, missing values are filled with the mean value. But as you can see below in pink, filling missing values with the mean value is not ideal. Our human intuition tells us that the time series values should not drop down to the mean value where the data points are missing. Instead the values should probably be filled with some value between 27 and 30.\n",
    "\n",
    "For this purpose, we can use the .ffill() method, which just uses the last observed value to fill the missing value.\n",
    "\n",
    "This is a popular approach to fill missing values in time series forecasting. It is also similar to the naive forecasting method. In naive forecasts, the forecast is simply the observed value. Despite its simplicity, the naive approach is a difficult baseline to beat. But think about it: If you want to forecast tomorrow's weather, a good guess would be that it will be similar to today. If it snowed today, it is quite unlikely that it is going to be hot tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402a2454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling na values by startdate and location group\n",
    "train = train.sort_values(by=['loc_group', 'startdate']).ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32870053",
   "metadata": {},
   "source": [
    "Need to create a time feature to account for data drift\n",
    "\n",
    "https://colab.research.google.com/drive/10r73mOp1R7cORfeuP97V65a-rgwGyfWr?usp=sharing#scrollTo=IEXmclobK12D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fbdc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time features\n",
    "\n",
    "def create_time_features(df):\n",
    "    df = df.copy()\n",
    "    #df['year'] = df.startdate.dt.year\n",
    "    df['quarter'] = df.startdate.dt.quarter\n",
    "    df['month'] = df.startdate.dt.month\n",
    "    df['week'] = df.startdate.dt.weekofyear\n",
    "    df['dayofyear'] = df.startdate.dt.day_of_year\n",
    "    return df\n",
    "\n",
    "traindf = create_time_features(train)\n",
    "testdf = create_time_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1140d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_season(df):\n",
    "  month_to_season = {\n",
    "      1: 0,\n",
    "      2: 0,\n",
    "      3: 1,\n",
    "      4: 1,\n",
    "      5: 1,\n",
    "      6: 2,\n",
    "      7: 2,\n",
    "      8: 2, \n",
    "      9: 3, \n",
    "      10: 3,\n",
    "      11: 3,\n",
    "      12: 0\n",
    "  }\n",
    "  df['season'] = df['month'].apply(lambda x: month_to_season[x])\n",
    "    \n",
    "add_season(traindf)\n",
    "add_season(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4768473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "\n",
    "def cos_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cyclical(df):\n",
    "  # encode the day with a period of 365\n",
    "  df['day_of_year_sin'] = sin_transformer(365).fit_transform(df['dayofyear'])\n",
    "  df['day_of_year_cos'] = cos_transformer(365).fit_transform(df['dayofyear'])\n",
    "\n",
    "  # encode the month with a period of 12\n",
    "  df['month_sin'] = sin_transformer(12).fit_transform(df['month'])\n",
    "  df['month_cos'] = cos_transformer(12).fit_transform(df['month'])\n",
    "\n",
    "  # encode the season with a period of 4\n",
    "  df['season_sin'] = sin_transformer(4).fit_transform(df['season'])\n",
    "  df['season_cos'] = cos_transformer(4).fit_transform(df['season'])\n",
    "\n",
    "encode_cyclical(traindf)\n",
    "encode_cyclical(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a70ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(7, 5))\n",
    "# plt.title(\"Graphical visualization of cyclical encoding\")\n",
    "# ax.scatter(traindf[\"month_sin\"], traindf[\"month_cos\"], c=traindf[\"month\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a8ad3",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "look at correlation and feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3afc6",
   "metadata": {},
   "source": [
    "#### Correlation\n",
    "\n",
    "Want to remove highly correlated features from model because of multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a55d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = traindf.drop(\"index\", axis=1).corr()\n",
    "f, ax = plt.subplots(figsize=(100,100))\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr,annot = True, mask = mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a1a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify correlated features to drop that fall above a correlation threshold \n",
    "## https://goodboychan.github.io/python/datacamp/machine_learning/2020/07/08/02-Feature-selection-I-selecting-for-feature-information.html \n",
    "def identify_correlated(df, threshold):\n",
    "    corr_matrix = df.corr().abs()\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    reduced_corr_matrix = corr_matrix.mask(mask)\n",
    "    features_to_drop = [c for c in reduced_corr_matrix.columns if any(reduced_corr_matrix[c] > threshold)]\n",
    "    return features_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the column names to drop \n",
    "to_drop = identify_correlated(traindf, threshold=.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2dfd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(traindf.drop(to_drop, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d029550e",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8823c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['contest-tmp2m-14d__tmp2m']\n",
    "X = df.drop(['index', 'startdate'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=312)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f3afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps = [\n",
    "              ('scaler', StandardScaler())\n",
    "              ,('regressor',RandomForestRegressor())\n",
    "           ])\n",
    "\n",
    "rf_model2 = pipeline.fit(x_train_reduced, y_train)\n",
    "\n",
    "train_preds = rf_model2.predict(x_train_reduced)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train,train_preds))\n",
    "print (f'Training Performance: {train_rmse}')\n",
    "train_score = r2_score(y_train, train_preds)\n",
    "print (f'Training Performance: {train_score}')\n",
    "\n",
    "\n",
    "test_preds = rf_model2.predict(x_test_reduced)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_train,test_preds))\n",
    "print (f'Test Performance: {test_rmse}')\n",
    "test_score = r2_score(y_test, test_preds)\n",
    "print (f'Test Performance: {test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a28dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the regression model\n",
    "regression = LinearRegression()\n",
    "\n",
    "# Fit the model on the PCA transformed data\n",
    "regression.fit(X_train, y_train)\n",
    "\n",
    "# Predict on new data\n",
    "y_pred = regression.predict(X_test)\n",
    "\n",
    "\n",
    "print(f'Intercept: {regression.intercept_:.3f}')\n",
    "\n",
    "print('Coefficients:')\n",
    "for name, coef in zip(X, regression.coef_):\n",
    "    print(f' {name}: {coef}')\n",
    "\n",
    "# Initialize k-fold cross-validator\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store the scores\n",
    "mae_scores = []\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_cv_train, X_cv_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_cv_train, y_cv_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    regression.fit(X_cv_train, y_cv_train)\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_pred = regression.predict(X_cv_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mae = mean_absolute_error(y_cv_test, y_pred)\n",
    "    mse = mean_squared_error(y_cv_test, y_pred)\n",
    "    r2 = r2_score(y_cv_test, y_pred)\n",
    "\n",
    "    # Append the scores to the lists\n",
    "    mae_scores.append(mae)\n",
    "    mse_scores.append(mse)\n",
    "    r2_scores.append(r2)\n",
    "    \n",
    "print('Mean Absolute Error:', np.mean(mae_scores))\n",
    "print('Mean Squared Error:', np.mean(mse_scores))\n",
    "print('R^2 Score:', np.mean(r2_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
